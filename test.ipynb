{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='9904b230-f3b8-40c4-9ff7-eafef507e661', metadata={'source': 'docs/LinkedIn Profile as of 20-25.pdf', 'page': 2, 'page_label': '3'}, page_content='foundation and transformative experiences I gained at Bristol, for which I am immensely grateful. \\nPost Aug/2023 \\nI have recently returned from the European Universities Futsal Championship in Split, Croatia, where \\nI had the privilege to represent the University of Bristol on an international stage. The experience \\nwas both; thoroughly enjoyable and one from which I learned a lot. \\nExperience: \\nSSE \\nData and Analytics GraduateData and Analytics Graduate'), Document(id='2545a0cc-d8de-4aa0-8ed9-c8cc4c1a231e', metadata={'source': 'docs/LinkedIn Profile as of 20-25.pdf', 'page': 1, 'page_label': '2'}, page_content='Playing for SUFC was incredibly enjoyable and offered me an outlook to make many friends outside \\nof my course.  \\n \\nI’m deeply grateful to the University of Southampton and ECS for their incredible support, and I’m \\nexcited to build on what I have learnt from my experiences. This course has helped equip me with a \\nskillset which I hope to utilise in industry to develop safe, scalable, and impactful AI. AI will be a'), Document(id='ead070ec-7c78-44e9-a273-304c6f0bfae4', metadata={'source': 'docs/LinkedIn Profile as of 20-25.pdf', 'page': 0, 'page_label': '1'}, page_content='associated with AI is a key skill for any AI practitioner and I am proud to have developed my \\nunderstanding of this area through my MSc.  \\n \\nOutside of academia, I also had the privilege of representing the university football team SUFC.'), Document(id='422ae889-7507-42b5-ad0c-8a519ec5615a', metadata={'source': 'docs/LinkedIn Profile as of 20-25.pdf', 'page': 2, 'page_label': '3'}, page_content=\"Mathematics department, allowing me to share my passion for the subject and contribute to the \\nlearning journey of fellow students. Serving as the Vice President of the Futsal society at the \\nUniversity helped me develop strong organisational, collaborative and leadership skills. \\n \\nI am excited to step into the next chapter of my education starting a masters in Artificial Intelligence \\nat the University of Southampton. This path wouldn't have been possible without the solid\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Document Paths\n",
    "cv_doc_path = \"docs/CV_june_24.pdf\"\n",
    "linkedin_doc_path = \"docs/LinkedIn Profile as of 20-25.pdf\"\n",
    "doc_paths = [cv_doc_path, linkedin_doc_path]\n",
    "\n",
    "# Load documents\n",
    "def load_docs(doc_paths):\n",
    "    docs = []\n",
    "    for file_path in doc_paths:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs.extend(loader.load())  # Extend to handle multiple pages properly\n",
    "    return docs\n",
    "\n",
    "# Load and split documents\n",
    "documents = load_docs(doc_paths)\n",
    "\n",
    "# Define a semantic character text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Apply splitter to documents\n",
    "split_docs = splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "retrieved_docs = vector_store.similarity_search(\"Where did oli go University\")\n",
    "print(retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Load embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant for answering questions about Oliver Simmonds (Oli). \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
    ")\n",
    "def ai_response(question):\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in vector_store.similarity_search(question))\n",
    "    messages = prompt_template.invoke({\"question\": question, \"context\": context})\n",
    "    response = llm.invoke(messages)\n",
    "    return response.split(\"Answer:\")[1].strip()\n",
    "\n",
    "response = ai_response('what university did oli go to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: • What degree of computer science was'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_20436\\3886360992.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load embeddings and vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant for answering questions about Oliver Simmonds (Oli). \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def ai_response(question):\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in vector_store.similarity_search(question))\n",
    "    messages = prompt_template.invoke({\"question\": question, \"context\": context})\n",
    "    response = llm.invoke(messages)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'University of Bristol'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ai_response(\"What university did oli go to\")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
